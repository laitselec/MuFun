import gradio as gr
import json
import time
import numpy as np
import os
import torch
import gc
import threading

from transformers import BitsAndBytesConfig, AutoTokenizer, AutoModelForCausalLM

# æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªç±»æ¥å¤„ç†æ¨¡å‹çš„åŠ è½½ã€å¸è½½å’ŒçŠ¶æ€ç®¡ç†

class ModelManager:
    def __init__(self, hf_path, unload_timeout=300):
        """
        åˆå§‹åŒ–æ¨¡å‹ç®¡ç†å™¨ã€‚
        :param hf_path: Hugging Face æ¨¡å‹è·¯å¾„ã€‚
        :param unload_timeout: æ— æ´»åŠ¨å¤šå°‘ç§’åå¸è½½æ¨¡å‹ï¼ˆç§’ï¼‰ã€‚
        """
        self.hf_path = hf_path
        self.unload_timeout = unload_timeout
        self.model = None
        self.tokenizer = None
        self.model_lock = threading.Lock()  # çº¿ç¨‹é”ï¼Œé˜²æ­¢å¹¶å‘åŠ è½½/å¸è½½å†²çª
        self.unload_timer = None
        print("ModelManager initialized. Model will be loaded on first request.")

    def _load_model(self):
        """å†…éƒ¨æ–¹æ³•ï¼šåŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨åˆ°æ˜¾å­˜"""
        if self.model is None:
            print("Loading model into VRAM...")
            start_time = time.time()
            try:
                self.tokenizer = AutoTokenizer.from_pretrained(self.hf_path, use_fast=False)
                quantization_config = BitsAndBytesConfig(
                    load_in_4bit=True,
                    llm_int8_skip_modules=["lm_head", 'vision_tower', 'connector']
                )
                self.model = AutoModelForCausalLM.from_pretrained(
                    self.hf_path,
                    trust_remote_code=True,
                    torch_dtype="bfloat16",
                    device_map="auto",
                    quantization_config=quantization_config
                )
                end_time = time.time()
                print(f"Model loaded successfully in {end_time - start_time:.2f} seconds.")
                print(f"Memory footprint: {self.model.get_memory_footprint()} bytes")
            except Exception as e:
                print(f"Error loading model: {e}")
                self.model = None
                self.tokenizer = None


    def _unload_model(self):
        """å†…éƒ¨æ–¹æ³•ï¼šä»æ˜¾å­˜å¸è½½æ¨¡å‹"""
        # åŠ é”ç¡®ä¿åœ¨å¸è½½æ—¶æ²¡æœ‰å…¶ä»–çº¿ç¨‹åœ¨è®¿é—®æ¨¡å‹
        with self.model_lock:
            if self.model is not None:
                print("Unloading model from VRAM...")
                start_time = time.time()
                del self.model
                del self.tokenizer
                self.model = None
                self.tokenizer = None
                gc.collect()  # åƒåœ¾å›æ”¶
                torch.cuda.empty_cache()  # æ¸…ç©ºPyTorchçš„CUDAç¼“å­˜
                end_time = time.time()
                print(f"Model unloaded successfully in {end_time - start_time:.2f} seconds.")
            # ç¡®ä¿å®šæ—¶å™¨è¢«æ¸…ç†
            self.unload_timer = None

    def get_model(self):
        """
        è·å–æ¨¡å‹å’Œåˆ†è¯å™¨ã€‚è¿™æ˜¯å¤–éƒ¨è°ƒç”¨çš„ä¸»è¦æ¥å£ã€‚
        å®ƒä¼šå¤„ç†æ¨¡å‹çš„åŠ è½½å’Œå¸è½½å®šæ—¶ã€‚
        """
        with self.model_lock:
            # å¦‚æœæœ‰æ­£åœ¨ç­‰å¾…çš„å¸è½½ä»»åŠ¡ï¼Œå–æ¶ˆå®ƒï¼Œå› ä¸ºæˆ‘ä»¬æ”¶åˆ°äº†æ–°è¯·æ±‚
            if self.unload_timer:
                self.unload_timer.cancel()
                print("New request received. Canceled a pending model unload.")

            # å¦‚æœæ¨¡å‹æœªåŠ è½½ï¼Œåˆ™åŠ è½½å®ƒ
            if self.model is None:
                self._load_model()

            # (é‡æ–°)è®¾ç½®å¸è½½å®šæ—¶å™¨
            print(f"Scheduling model unload in {self.unload_timeout} seconds.")
            self.unload_timer = threading.Timer(self.unload_timeout, self._unload_model)
            self.unload_timer.start()

            return self.model, self.tokenizer

hf_path = 'Yi3852/MuFun-ACEStep'
# è®¾ç½®è¶…æ—¶æ—¶é—´ä¸º 300 ç§’ï¼ˆ5åˆ†é’Ÿï¼‰
model_manager = ModelManager(hf_path, unload_timeout=300)

inp = '<audio>\nDeconstruct this song, listing its tags and lyrics. Directly output a JSON object with prompt and lyrics fields, without any additional explanations or text.'

EXAMPLE_DIR = "examples"
if not os.path.exists(EXAMPLE_DIR):
    os.makedirs(EXAMPLE_DIR)
example_path_1 = os.path.join(EXAMPLE_DIR, "ace_output.wav")
example_path_2 = os.path.join(EXAMPLE_DIR, "yesterday.flac")
example_path_3 = os.path.join(EXAMPLE_DIR, "æ¢¦ä¸­äºº.mp3")


def generate_music_data(audio_filepath):
    if not audio_filepath:
        return "Please upload an audio file first.", ""

    print(f"Received request for audio file: {audio_filepath}")
    
    # ä»ç®¡ç†å™¨è·å–æ¨¡å‹ï¼Œè¿™æ­¥ä¼šè‡ªåŠ¨å¤„ç†åŠ è½½å’Œå®šæ—¶å¸è½½
    model, tokenizer = model_manager.get_model()
    
    # æ£€æŸ¥æ¨¡å‹æ˜¯å¦æˆåŠŸåŠ è½½
    if model is None or tokenizer is None:
        error_msg = "Model could not be loaded. Please check the logs."
        return error_msg, error_msg
        
    try:
        # ä½¿ç”¨è·å–åˆ°çš„æ¨¡å‹è¿›è¡Œæ¨ç†
        res = model.chat(prompt=inp, audio_files=audio_filepath, segs=None, tokenizer=tokenizer, temperature=0.2)
        print(f"Model response: {res}")
        json_output = json.loads(res)
    except Exception as e:
        print(f"Error during model inference: {e}")
        return "Error during model inference", str(e)
    
    prompt = json_output.get("prompt", "N/A")
    lyrics = json_output.get("lyrics", "N/A")
    
    return prompt, lyrics

with gr.Blocks(theme=gr.themes.Soft()) as demo:
    gr.Markdown(
        """
        # ğŸµ Music to Prompt & Lyrics Generator
        Upload a song or select an example below. The model will analyze the audio 
        and generate a descriptive prompt and the corresponding lyrics.
        **Note:** The model will be automatically loaded on the first request and unloaded after 5 minutes of inactivity to save VRAM.
        """
    )
    
    with gr.Row():
        with gr.Column(scale=1):
            audio_input = gr.Audio(
                type="filepath", 
                label="Upload Your Song",
                sources=["upload", "microphone"]
            )
            gr.Examples(
                examples=[example_path_1, example_path_2, example_path_3],
                inputs=audio_input,
                label="Example Songs"
            )
            submit_btn = gr.Button("âœ¨ Generate", variant="primary")

        with gr.Column(scale=2):
            prompt_output = gr.Textbox(
                label="Generated Prompt",
                info="A set of descriptive tags for the music."
            )
            lyrics_output = gr.Textbox(
                label="Generated Lyrics", 
                lines=15,
                info="The lyrics for the music."
            )

    submit_btn.click(
        fn=generate_music_data,
        inputs=audio_input,
        outputs=[prompt_output, lyrics_output]
    )

if __name__ == "__main__":
    demo.launch(share=True, allowed_paths=["."])